{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPk+ZPk+JvodXdQn/iU/02i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sammylayo/SammyPortfolioProject/blob/main/TensorFlow_basic_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Sn3s2BBGr1ym"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dataset = tf.keras.utils.get_file(\"aclImdb_v1\", url, untar=True, cache_dir='.', cache_subdir=''): The tf.keras.utils.get_file() function is used to download and extract the dataset. It has several arguments:\n",
        "\n",
        "\"aclImdb_v1\": This is the name that will be given to the downloaded file. It serves as the base name for the file when it is saved to disk.\n",
        "\n",
        "url: The URL of the dataset to be downloaded.\n",
        "\n",
        "untar=True: The untar argument is set to True, indicating that the downloaded file should be extracted. Since the dataset is in the form of a .tar.gz file, it will be extracted after downloading.\n",
        "\n",
        "cache_dir='.': The cache_dir argument specifies the directory where the downloaded file will be stored. In this case, '.' represents the current working directory.\n",
        "\n",
        "cache_subdir='': The cache_subdir argument specifies a subdirectory within the cache directory where the file will be saved. In this case, an empty string means that the file will be saved directly in the cache directory without any subdirectory.\n",
        "\n",
        "The dataset variable will hold the path to the downloaded and extracted dataset.\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb'): The os.path.join() function is used to create the path to the directory where the dataset is extracted. It combines the parent directory of the dataset file (i.e., the cache directory) with the subdirectory 'aclImdb'. The resulting dataset_dir variable will contain the path to the directory where the IMDb movie reviews dataset is extracted"
      ],
      "metadata": {
        "id": "Q0b87NEgwDPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"aclImdb_v1\", url,\n",
        "                                    untar=True, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')"
      ],
      "metadata": {
        "id": "FU6xRhrd05Uo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ed80ddd-48a6-46bc-fb94-0286bc7140c5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "84125825/84125825 [==============================] - 12s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.path.dirname(dataset) ##which is the current directory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3kIV3DK2r1BT",
        "outputId": "0c262053-dec3-4c75-e531-3412e2b3d85b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(dataset_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yw4xb-B-1HAH",
        "outputId": "b7c94cdc-a650-4b48-c4e0-cce6cf580652"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['train', 'test', 'README', 'imdbEr.txt', 'imdb.vocab']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = os.path.join(dataset_dir, 'train')\n",
        "os.listdir(train_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGOq6AAY2I40",
        "outputId": "7d00a545-9722-4a53-d9bf-b8012b3e2856"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['neg',\n",
              " 'urls_unsup.txt',\n",
              " 'pos',\n",
              " 'urls_pos.txt',\n",
              " 'urls_neg.txt',\n",
              " 'unsupBow.feat',\n",
              " 'labeledBow.feat']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_dir = os.path.join(train_dir, 'pos')\n",
        "text = os.listdir(pos_dir)[0]\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "u14Bpakg5d2r",
        "outputId": "025c823b-c483-458d-b7a9-4eb825020a06"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1769_8.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_file = os.path.join(pos_dir, text)\n",
        "with open(sample_file) as sammy:\n",
        "  print(sammy.read())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggwJCdSr3jX_",
        "outputId": "cfee8237-c16e-478d-bb9b-ddc720728189"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Just came out of a sneak preview for this film. It had me laughing every 30 secs. The ending was so funny that tears were rolling down my face and it had me wishing I hadn't bought that large coke. There are definitely some lulls, but, overall, highly entertaining. The movie lets Steve Carell have a chance to shine after stealing the spotlight from both Jim Carrey in \"Bruce Almighty\" and Will Ferrell \"Anchorman: The Legend of Ron Burgendy\" in their movies. Paul Rudd is hilarious as always. I love that he can be so funny in these broad comedies and continues to work in indie dramas (like P.S.). I think that Seth Rogen should be getting more work, because he so freaking talented and engaging. Leslie Mann also had some incredibly funny moments. I highly recommend it for those who just want to laugh like a maniac. However, if you're easily offended, don't see this movie. If you're a rabid feminist, don't see this movie. And, please, not matter what, even if you think you're one of those \"hip\" parents, don't take your kids to this movie. Sure, you should let your teens go see this movie, just don't watch it with them. It would make for some incredibly awkward moments.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "remove_dir = os.path.join(train_dir, 'unsup')\n",
        "shutil.rmtree(remove_dir)"
      ],
      "metadata": {
        "id": "OL_B5jZI4Dvp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "seed = 42\n",
        "\n",
        "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    'aclImdb/train/',\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    seed=seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nu0EjVJ77xRy",
        "outputId": "f68cced7-46f0-40f4-88d4-196ab3df54cb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(raw_train_ds.as_numpy_iterator())[0][0][0].strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCUfbDq5rxX-",
        "outputId": "cd80f647-0a8a-4597-f813-684945a3b84b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'Silent Night, Deadly Night 5 is the very last of the series, and like part 4, it\\'s unrelated to the first three except by title and the fact that it\\'s a Christmas-themed horror flick.<br /><br />Except to the oblivious, there\\'s some obvious things going on here...Mickey Rooney plays a toymaker named Joe Petto and his creepy son\\'s name is Pino. Ring a bell, anyone? Now, a little boy named Derek heard a knock at the door one evening, and opened it to find a present on the doorstep for him. Even though it said \"don\\'t open till Christmas\", he begins to open it anyway but is stopped by his dad, who scolds him and sends him to bed, and opens the gift himself. Inside is a little red ball that sprouts Santa arms and a head, and proceeds to kill dad. Oops, maybe he should have left well-enough alone. Of course Derek is then traumatized by the incident since he watched it from the stairs, but he doesn\\'t grow up to be some killer Santa, he just stops talking.<br /><br />There\\'s a mysterious stranger lurking around, who seems very interested in the toys that Joe Petto makes. We even see him buying a bunch when Derek\\'s mom takes him to the store to find a gift for him to bring him out of his trauma. And what exactly is this guy doing? Well, we\\'re not sure but he does seem to be taking these toys apart to see what makes them tick. He does keep his landlord from evicting him by promising him to pay him in cash the next day and presents him with a \"Larry the Larvae\" toy for his kid, but of course \"Larry\" is not a good toy and gets out of the box in the car and of course, well, things aren\\'t pretty.<br /><br />Anyway, eventually what\\'s going on with Joe Petto and Pino is of course revealed, and as with the old story, Pino is not a \"real boy\". Pino is probably even more agitated and naughty because he suffers from \"Kenitalia\" (a smooth plastic crotch) so that could account for his evil ways. And the identity of the lurking stranger is revealed too, and there\\'s even kind of a happy ending of sorts. Whee.<br /><br />A step up from part 4, but not much of one. Again, Brian Yuzna is involved, and Screaming Mad George, so some decent special effects, but not enough to make this great. A few leftovers from part 4 are hanging around too, like Clint Howard and Neith Hunter, but that doesn\\'t really make any difference. Anyway, I now have seeing the whole series out of my system. Now if I could get some of it out of my brain. 4 out of 5.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(next(iter(raw_train_ds))[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLZdiDnR7WtA",
        "outputId": "97e8d881-559f-4bb5-8812-aa236e5cda0f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(next(iter(raw_train_ds)))[1][2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipxhYOlg13Ps",
        "outputId": "baa4c158-d25a-4ee0-b9ea-8aebcaae143c"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=int32, numpy=0>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(raw_train_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "FKXvnBsl90a5",
        "outputId": "03e8c762-1268-4b52-d3f2-1d8a9c4e3fcf"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensorflow.python.data.ops.batch_op._BatchDataset"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>tensorflow.python.data.ops.batch_op._BatchDataset</b><br/>def __init__(input_dataset, batch_size, drop_remainder, name=None)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/batch_op.py</a>A `Dataset` that batches contiguous elements from its input.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 50);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for text_batch, label_batch in raw_train_ds.take(1):\n",
        "  for i in range(1):\n",
        "    print(\"Review\", text_batch.numpy()[i])\n",
        "    print(\"Label\", label_batch.numpy()[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywvH6xq389RA",
        "outputId": "64f1a570-0cad-4950-ccbf-c25d23e2185c"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review b'Skippy from \"Family Ties\" plays Eddie, a wussy \\'metal\\' nerd who gets picked on. When his favorite wussy \\'metal\\' singer, Sammi Curr, dies, he throws a hissy fit tearing down all the posters on his bedroom wall. But when he later gets an unreleased record that holds the spirit of his dead \\'metal\\' idol. He first gets sucked into ideas of revenge, but then he doesn\\'t want to take it as far as Sammi does. Which isn\\'t really that far as his main victims only seem to go to the hospital. This movie is utterly laughable and has about as much to do with real metal as say, \"Rock Star\". OK, maybe a tad more than that piece of junk, but you get my point. And how ANYone can root for a guy played by Skippy from \"Family Ties\" I haven\\'t a clue. The cameo by Gene Simmons is OK, and Ozzy Osbourne reaches coherency, I applaud him for that, but otherwise skip this one.<br /><br />My Grade: D <br /><br />Eye Candy:Elise Richards gets topless, an a topless extra at a pool party'\n",
            "Label 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(text_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IgOylwJAmbh",
        "outputId": "35294416-0098-4a8b-8ac8-f81efa9a8dac"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Label 0 corresponds to\", raw_train_ds.class_names[0])\n",
        "print(\"Label 1 corresponds to\", raw_train_ds.class_names[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hADq6NeHApow",
        "outputId": "8d5c21ee-90fc-4365-8298-c5993b3fa52e"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label 0 corresponds to neg\n",
            "Label 1 corresponds to pos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_val_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    'aclImdb/train',\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='validation',\n",
        "    seed=seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCaw3XqNCBNB",
        "outputId": "c687b0af-f7ac-474a-a41f-d5910a7d8b69"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_test_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    'aclImdb/test',\n",
        "    batch_size=batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-DbsmgjCKBj",
        "outputId": "91427f38-07d0-465e-c4c8-361bbe57f519"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The custom_standardization function takes input_data as an argument, which represents a text string or a tensor of text data. The function applies the following operations for text standardization:\n",
        "\n",
        "Lowercasing: The tf.strings.lower() function is used to convert the text to lowercase. This helps normalize the text by removing case distinctions.\n",
        "\n",
        "Stripped HTML: The tf.strings.regex_replace() function is used to replace the <br /> HTML tag with a space. This is typically done to handle HTML content where <br /> represents line breaks.\n",
        "\n",
        "Punctuation Removal: The tf.strings.regex_replace() function is again used to remove punctuation marks from the text. The regular expression [%s]' % re.escape(string.punctuation) matches any punctuation characters defined in the string.punctuation module from the text and replaces them with an empty string.\n",
        "\n",
        "The function returns the processed text tensor after applying these standardization operations.\n",
        "\n",
        "You can use this custom_standardization function as a preprocessing step for text data in your TensorFlow models, such as tokenization or embedding layers, to ensure consistent and standardized input representation."
      ],
      "metadata": {
        "id": "NaKGxJJZD4Kk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
        "  return tf.strings.regex_replace(stripped_html,\n",
        "                                  '[%s]' % re.escape(string.punctuation),\n",
        "                                  '')"
      ],
      "metadata": {
        "id": "ghEqAr1yCX2M"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "standardize: This parameter refers to a function that performs custom text standardization or preprocessing. In the provided code, custom_standardization is passed as the value. This function is applied to each input text before tokenization and vectorization.\n",
        "\n",
        "max_tokens: This parameter determines the maximum number of tokens (unique words) to consider when tokenizing the text data. In this case, max_features is used as the value, indicating that the tokenizer should consider the top 10,000 most frequent tokens.\n",
        "\n",
        "output_mode: This parameter specifies the output mode of the TextVectorization layer. The value 'int' indicates that the layer should output integer representations of the tokens.\n",
        "\n",
        "output_sequence_length: This parameter determines the fixed sequence length for the output tensor. It specifies the number of tokens in each sequence by padding or truncating the input text. In this case, sequence_length is set to 250, indicating that each input sequence will have a length of 250 tokens.\n",
        "\n",
        "By creating the TextVectorization layer with these configurations, you can use it to preprocess and vectorize text data in your TensorFlow models. The layer standardizes the text, tokenizes it, converts the tokens to integers, and ensures a consistent sequence length for the input data."
      ],
      "metadata": {
        "id": "kExfud6OGD4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 15000\n",
        "sequence_length = 500\n",
        "\n",
        "vectorize_layer = layers.TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=max_features,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length, )"
      ],
      "metadata": {
        "id": "besYif-9D6Hp"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "train_text = raw_train_ds.map(lambda x, y: x): This line of code creates a new dataset train_text by mapping the raw_train_ds dataset using a lambda function. The lambda function (lambda x, y: x) extracts only the text (x) from each element of the raw_train_ds dataset, discarding the labels (y).\n",
        "\n",
        "vectorize_layer.adapt(train_text): This line adapts the vectorize_layer to the train_text dataset. The adapt() method analyzes the dataset and adjusts the internal state of the layer accordingly. This adaptation process enables the layer to build the vocabulary of tokens based on the text in the training dataset.\n",
        "\n",
        "By calling adapt() on the vectorize_layer with the train_text dataset, the layer learns the vocabulary of tokens and their indices. It determines the unique tokens present in the training text data, assigns indices to them, and sets up the necessary internal mappings to convert text data to vectorized representations.\n",
        "\n",
        "After adapting the vectorize_layer, it can be used to preprocess and vectorize text data consistently and effectively in subsequent stages, such as model training or evaluation."
      ],
      "metadata": {
        "id": "1NbRvTm_Miex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a text-only dataset (without labels), then call adapt\n",
        "train_text = raw_train_ds.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(train_text)"
      ],
      "metadata": {
        "id": "42ZFoHW8F3ip"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "text = tf.expand_dims(text, -1): This line of code uses tf.expand_dims() to add an extra dimension to the text tensor. This is done to match the expected input shape of the vectorize_layer. The -1 argument indicates adding the new dimension as the last dimension of the tensor.\n",
        "\n",
        "return vectorize_layer(text), label: This line returns the result of applying the vectorize_layer to the expanded text tensor, along with the original label. The vectorize_layer(text) call applies the text vectorization process defined by the TextVectorization layer to the text tensor, converting it into a vectorized representation. The resulting tensor is returned along with the original label.\n",
        "\n",
        "By using this vectorize_text function, you can conveniently preprocess and vectorize text data along with their corresponding labels in a consistent manner. This function is typically used when preparing input data for training or evaluation in a TensorFlow model."
      ],
      "metadata": {
        "id": "vP7uolvlJOcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return vectorize_layer(text), label"
      ],
      "metadata": {
        "id": "enP3z4S-Gxi3"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "next(iter(raw_train_ds)): This line uses the iter() function to create an iterator from the raw_train_ds TensorFlow Dataset object. The next() function is then called to retrieve the next element from the iterator, which corresponds to a batch of samples."
      ],
      "metadata": {
        "id": "HJ-_57JHwSUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "exp_no = tf.expand_dims(no, 2)#The 2nd positional argument determines where the expansion will take place... note that the 2 in this case causes the shape to have 1 in the 3rd position...\n",
        "print(exp_no)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xjmmCdbCo8r",
        "outputId": "ccae4c79-a7c8-4b7a-f7c3-704248573094"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]]], shape=(2, 2, 1), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieve a batch (of 32 reviews and labels) from the dataset\n",
        "text_batch, label_batch = next(iter(raw_train_ds))\n",
        "first_review, first_label = text_batch[0], label_batch[0]\n",
        "print(\"Review\", first_review.numpy())\n",
        "print(\"Label\", raw_train_ds.class_names[first_label])\n",
        "print(\"Vectorized review\", vectorize_text(first_review, first_label))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etPf8A_9Hz9z",
        "outputId": "7c9c62cb-efbf-4c72-9596-72686ce301e0"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review b'Uta Hagen\\'s \"Respect for Acting\" is the standard textbook in many college theater courses. In the book, Hagen presents two fundamentally different approaches to developing a character as an actor: the Presentational approach, and the Representational approach. In the Presentational approach, the actor focuses on realizing the character as honestly as possible, by introducing emotional elements from the actor\\'s own life. In the Representational approach, the actor tries to present the effect of an emotion, through a high degree of control of movement and sound.<br /><br />The Representational approach to acting was still partially in vogue when this Hamlet was made. British theater has a long history of this style of acting, and Olivier could be said to be the ultimate king of the Representational school.<br /><br />Time has not been kind to this school of acting, or to this movie. Nearly every working actor today uses a Presentational approach. To the modern eye, Olivier\\'s highly enunciated, stylized delivery is stodgy, stiff and stilted. Instead of creating an internally conflicted Hamlet, Olivier made a declaiming, self-important bullhorn out of the melancholy Dane -- an acting style that would have carried well to the backs of the larger London theaters, but is far too starchy to carry off a modern Hamlet.<br /><br />And so the movie creaks along ungainfully today. Olivier\\'s tendency to e-nun-ci-ate makes some of Hamlet\\'s lines unintentionally funny: \"In-stead, you must ac-quire and be-get a tem-purr-ance that may give it... Smooth-ness!\" Instead of crying at meeting his father\\'s ghost (as any proper actor could), bright fill lights in Olivier\\'s pupils give us that impression.<br /><br />Eileen Herlie is the only other actor of note in this Hamlet, putting in a good essay at the Queen, despite the painfully obvious age differences (he was 41; she was 26). The other actors in this movie have no chance to get anything else of significance done, given Olivier\\'s tendency to want to keep! the camera! on him! at all! times! <br /><br />Sixty years later, you feel the insecurity of the Shakespearean stage actor who lacked the confidence to portray a breakable, flawed Hamlet, and instead elected to portray a sort of Elizabethan bullhorn. Final analysis: \"I would have such a fellow whipped for o\\'er-doing Termagant; it out-herods Herod: pray you, avoid it.\"'\n",
            "Label neg\n",
            "Vectorized review (<tf.Tensor: shape=(1, 500), dtype=int64, numpy=\n",
            "array([[    1,     1,  1138,    15,   109,     7,     2,  1200,  9683,\n",
            "            8,   107,  1199,   739,     1,     8,     2,   267,  9824,\n",
            "         2391,   104,     1,   263,  6452,     6,  3980,     4,   106,\n",
            "           14,    33,   285,     2,     1,  1418,     3,     2,     1,\n",
            "         1418,     8,     2,     1,  1418,     2,   285,  2670,    20,\n",
            "         4004,     2,   106,    14,  1182,    14,   611,    32,  6513,\n",
            "          865,   768,    35,     2,   151,   196,   116,     8,     2,\n",
            "            1,  1418,     2,   285,   494,     6,  1006,     2,   923,\n",
            "            5,    33,  1340,   140,     4,   331,  2435,     5,  1141,\n",
            "            5,  2422,     3,   477,     2,     1,  1418,     6,   109,\n",
            "           13,   125,  5586,     8, 11573,    51,    11,  3100,    13,\n",
            "           90,   681,   739,    43,     4,   209,   468,     5,    11,\n",
            "          433,     5,   109,     3,  4657,    98,    26,   292,     6,\n",
            "           26,     2,  1979,   683,     5,     2,     1,   413,    58,\n",
            "           43,    21,    74,   236,     6,    11,   413,     5,   109,\n",
            "           41,     6,    11,    17,   749,   165,   773,   285,   637,\n",
            "         1058,     4,     1,  1418,     6,     2,   709,   785, 14278,\n",
            "          540,     1,  6583,  2515,     7,     1,  3718,     3,  4404,\n",
            "          291,     5,  1754,    33,     1,  9608,  3100,  4657,    90,\n",
            "            4,     1,     1,     1,    44,     5,     2,  7356,  9371,\n",
            "           33,   109,   433,    12,    59,    25,  2932,    73,     6,\n",
            "            2,  6344,     5,     2,  3520,  1256,  2224,    18,     7,\n",
            "          231,    99,     1,     6,  1562,   127,     4,   709,  3100,\n",
            "            3,    37,     2,    17,     1,   358,     1,   637, 14278,\n",
            "         6679,     6,     1,   158,    46,     5,     1,   402,  3191,\n",
            "          160,   291,    22,   219,  9178,     3,     1,     4,     1,\n",
            "           12,   194,   193,     9,     1,   291,     5,  2713,    31,\n",
            "         2197,    24,  2034,  1201,    14,    97,  2154,   285,    98,\n",
            "         1907,  2213,  2646,     8, 14278, 13067,   193,   167,    12,\n",
            "         1314, 12764,     1,     7,     2,    61,    78,   285,     5,\n",
            "          870,     8,    11,  3100,  1501,     8,     4,    49, 12755,\n",
            "           31,     2,  1573,   462,     2,  2124,   576,   588,  3979,\n",
            "           27,    13,     1,    55,    13,  8154,     2,    78,   151,\n",
            "            8,    11,    17,    25,    57,   575,     6,    75,   228,\n",
            "          320,     5,  4819,   223,   340, 14278,  6679,     6,   178,\n",
            "            6,   372,     2,   379,    20,    85,    31,    30,   206,\n",
            "        10923,   149,   298,    22,   232,     2,     1,     5,     2,\n",
            "         9461,   895,   285,    36,  3404,     2,  4847,     6,  1829,\n",
            "            4,     1,  2980,  3100,     3,   291,  8295,     6,  1829,\n",
            "            4,   425,     5,     1,     1,   465,  5080,    10,    59,\n",
            "           25,   135,     4,  1476,  8753,    15,     1,     1,     9,\n",
            "            1,     1,  6090,    22,   774,     9,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0]])>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"1287 ---> \",vectorize_layer.get_vocabulary()[1287])\n",
        "print(\" 313 ---> \",vectorize_layer.get_vocabulary()[313])\n",
        "print('Vocabulary size: {}'.format(len(vectorize_layer.get_vocabulary())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iurmj8SmJgEK",
        "outputId": "6f9568df-5f39-4d43-a82e-314dc161221c"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1287 --->  silent\n",
            " 313 --->  night\n",
            "Vocabulary size: 15000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = raw_train_ds.map(vectorize_text)\n",
        "val_ds = raw_val_ds.map(vectorize_text)\n",
        "test_ds = raw_test_ds.map(vectorize_text)"
      ],
      "metadata": {
        "id": "IrGNPqqRJl4h"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE: AUTOTUNE is a special value in TensorFlow's tf.data module that allows the system to automatically tune the buffer size based on available system resources. It ensures optimal performance by dynamically adjusting the buffer size for reading and processing data.\n",
        "\n",
        "train_ds.cache(): The cache() method caches the training dataset in memory after the first epoch. This caching helps avoid reading and preprocessing the training data from disk in each epoch, significantly improving the training performance for subsequent epochs.\n",
        "\n",
        "train_ds.prefetch(buffer_size=AUTOTUNE): The prefetch() method prefetches data from the training dataset. It allows the data loading and preprocessing steps to overlap with model training, utilizing the available CPU/GPU resources efficiently. The buffer_size parameter specifies the number of elements to prefetch, and AUTOTUNE is used to automatically determine an appropriate buffer size based on the available system resources."
      ],
      "metadata": {
        "id": "uuGZWnvhW_B5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "dA2jz9F5Jxxx"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "layers.Embedding(max_features + 1, embedding_dim): The first layer in the model is an Embedding layer. This layer is responsible for learning word embeddings, which are dense vector representations of words in the input text. The max_features + 1 is the input size, representing the vocabulary size, and embedding_dim is the dimensionality of the word embeddings.\n",
        "\n",
        "layers.Dropout(0.2): The first dropout layer is used to prevent overfitting by randomly setting a fraction of input units to zero during training. The dropout rate is set to 0.2, meaning 20% of the input units will be randomly dropped during training."
      ],
      "metadata": {
        "id": "7LUiXvrgeCu5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 16\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  layers.Embedding(max_features + 1, embedding_dim),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.GlobalAveragePooling1D(),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.Dense(1)], )\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9QvVeWnJ0NE",
        "outputId": "5c372cf1-33b0-467e-a804-45555d2282c9"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 16)          240016    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, None, 16)          0         \n",
            "                                                                 \n",
            " global_average_pooling1d (  (None, 16)                0         \n",
            " GlobalAveragePooling1D)                                         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 240033 (937.63 KB)\n",
            "Trainable params: 240033 (937.63 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer='adam',\n",
        "              metrics=tf.metrics.BinaryAccuracy(threshold=0.0))"
      ],
      "metadata": {
        "id": "P35ePvyRJ7iy"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs,\n",
        "    verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0Kb6hX8KCeu",
        "outputId": "e1af77ef-fb8a-4284-dc98-d9d8a87c9d0b"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "625/625 [==============================] - 12s 17ms/step - loss: 0.6792 - binary_accuracy: 0.6449 - val_loss: 0.6543 - val_binary_accuracy: 0.6970\n",
            "Epoch 2/5\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.6045 - binary_accuracy: 0.7742 - val_loss: 0.5633 - val_binary_accuracy: 0.7990\n",
            "Epoch 3/5\n",
            "625/625 [==============================] - 7s 12ms/step - loss: 0.5112 - binary_accuracy: 0.8245 - val_loss: 0.4840 - val_binary_accuracy: 0.8374\n",
            "Epoch 4/5\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.4381 - binary_accuracy: 0.8532 - val_loss: 0.4258 - val_binary_accuracy: 0.8524\n",
            "Epoch 5/5\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.3849 - binary_accuracy: 0.8719 - val_loss: 0.3844 - val_binary_accuracy: 0.8642\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "export_model = tf.keras.Sequential([\n",
        "  vectorize_layer,\n",
        "  model,\n",
        "  layers.Activation('sigmoid')\n",
        "])\n",
        "\n",
        "export_model.compile(\n",
        "    loss=losses.BinaryCrossentropy(from_logits=False), optimizer=\"adam\", metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Test it with `raw_test_ds`, which yields raw strings\n",
        "loss, accuracy = export_model.evaluate(raw_test_ds)\n",
        "print(accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGxI0_CRQNY8",
        "outputId": "553d285a-0e4f-4809-8978-4a46c5805345"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 13s 16ms/step - loss: 0.3944 - accuracy: 0.8596\n",
            "0.8595600128173828\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [\n",
        "  \"The movie was very great wonderful!\",\n",
        "  \"The movie was okay.\",\n",
        "  \"The movie was very terrible...\",\n",
        "  \"The movie was the best film ever made\",\n",
        "  \"horrible\"\n",
        "]\n",
        "\n",
        "export_model.predict(examples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CugXF79KKE1g",
        "outputId": "c535540f-662e-4caf-a85e-f011c5235ded"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 154ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.6051175 ],\n",
              "       [0.4705807 ],\n",
              "       [0.4675867 ],\n",
              "       [0.51737005],\n",
              "       [0.46485427]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    }
  ]
}